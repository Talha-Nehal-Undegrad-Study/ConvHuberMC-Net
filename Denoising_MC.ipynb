{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Libraries\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "# from torchinfo import summary\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# import collections # A module providing alternative data structures like named tuples, defaultdict, Counter, etc., compared to built-in Python containers.\n",
    "# import random\n",
    "\n",
    "# Data Visualization\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn\n",
    "\n",
    "# File and System Interaction\n",
    "# import glob\n",
    "import os\n",
    "# from pathlib import Path\n",
    "# import shutil\n",
    "\n",
    "# # Scientific Computing and Math\n",
    "# import math\n",
    "# import cmath\n",
    "\n",
    "# # Date and Time Handling\n",
    "# import time\n",
    "# import datetime\n",
    "\n",
    "# Linear Algebra\n",
    "# from torch import linalg as LA\n",
    "from python_scripts import generate_synthetic_data\n",
    "from python_scripts import format_data\n",
    "from python_scripts import dataset_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd().replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_train.shape: (60, 1, 160, 320), M_Omega_train.shape: (60, 1, 160, 320), M_test.shape: (40, 1, 160, 320), M_Omega_test.shape: (40, 1, 160, 320)\n"
     ]
    }
   ],
   "source": [
    "# Generate Data\n",
    "M_train, M_Omega_train, M_test, M_Omega_test = generate_synthetic_data.generate(160, 320, 10, 60, 40, 0.45, 9)\n",
    "\n",
    "# Each training/test set is of the shape (batch_size, width, height)\n",
    "# Convert it to (batch_size, 1, width, height)\n",
    "\n",
    "M_train, M_Omega_train, M_test, M_Omega_test = M_train[:, np.newaxis, :, :], M_Omega_train[:, np.newaxis, :, :], M_test[:, np.newaxis, :, :], M_Omega_test[:, np.newaxis, :, :]\n",
    "\n",
    "# M_test = M_train[40:, :, :, :]\n",
    "# M_Omega_test = M_Omega_train[40:, :, :, :]\n",
    "\n",
    "# M_train = M_train[:40, :, :, :]\n",
    "# M_Omega_train = M_Omega_train[:40, :, :, :]\n",
    "\n",
    "print(f'M_train.shape: {M_train.shape}, M_Omega_train.shape: {M_Omega_train.shape}, M_test.shape: {M_test.shape}, M_Omega_test.shape: {M_Omega_test.shape}')\n",
    "\n",
    "# Format and Save Data\n",
    "format_data.format(M_train, M_Omega_train, M_test, M_Omega_test, ROOT)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = dataset_processing.ImageDataset(40, (160, 320), 0, ROOT)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size = 5, shuffle = True)\n",
    "test_dataset = dataset_processing.ImageDataset(20, (160, 320), 1, ROOT)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-5.2520981 ,  3.29874653, -2.75110075, ..., -2.16120863,\n",
       "           0.45127096,  1.3473237 ],\n",
       "         [-0.53716509,  0.18786701, -0.64480302, ...,  0.61963347,\n",
       "           2.50468989, -4.39335438],\n",
       "         [ 9.00988915, -1.29901776,  0.69608121, ..., -3.21938885,\n",
       "          -2.66594354, -7.64561896],\n",
       "         ...,\n",
       "         [ 2.468953  , -2.99122423, -2.91148951, ..., -0.7508852 ,\n",
       "          -1.76242488, -2.99220304],\n",
       "         [ 2.99106353,  0.09230885,  2.39664065, ...,  0.16009176,\n",
       "          -0.75106216,  0.80562855],\n",
       "         [ 0.55797921,  2.14034237, -1.2160436 , ...,  0.33526207,\n",
       "          -1.80622466, -3.83545331]]],\n",
       "\n",
       "\n",
       "       [[[ 4.23004683, -4.92988191, -4.14422318, ..., -0.21570829,\n",
       "           2.67066984,  4.57990939],\n",
       "         [ 2.83741192,  0.51620955, -6.5845917 , ...,  0.4145643 ,\n",
       "          -1.08836797,  0.16318017],\n",
       "         [-4.48819567, -5.43154374,  3.00322484, ...,  5.44052929,\n",
       "           5.85834154, -2.72445351],\n",
       "         ...,\n",
       "         [ 3.62571166,  2.35581326, -0.61872231, ..., -7.13912363,\n",
       "           1.3832218 ,  6.90506488],\n",
       "         [-1.97848045,  0.58430961, -0.86676475, ...,  0.20322395,\n",
       "          -6.61619401,  2.58648714],\n",
       "         [-3.38486462, -1.35527033,  0.37982922, ..., -2.30190818,\n",
       "          -1.66866564, -1.9965289 ]]],\n",
       "\n",
       "\n",
       "       [[[ 3.3872422 ,  0.12042332, -2.29199871, ..., -3.59828227,\n",
       "          -3.01983124,  4.32427098],\n",
       "         [ 3.1928662 ,  4.98009148,  2.45509257, ...,  5.17877876,\n",
       "          -2.20912546,  0.76719539],\n",
       "         [ 3.61596424,  5.26562122, -0.22131526, ..., -0.44149052,\n",
       "          -3.12318361,  4.6872694 ],\n",
       "         ...,\n",
       "         [-3.82610923,  0.2475061 ,  0.03300998, ..., -1.46469348,\n",
       "          -5.64811038,  7.97810148],\n",
       "         [-3.4390123 , -2.38549115,  0.4795798 , ...,  4.01448807,\n",
       "           1.56680383,  2.01325246],\n",
       "         [ 4.49633011,  2.79375075,  1.11996082, ...,  0.61173206,\n",
       "          -1.58317967, -4.29369213]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[-3.88643727, -0.12836246, -1.7480147 , ...,  2.83782466,\n",
       "           5.45275712,  5.34942998],\n",
       "         [ 0.94515868, -1.0975817 ,  2.07320114, ...,  1.77909736,\n",
       "          -2.14575188, -1.2209467 ],\n",
       "         [-1.96653085,  3.2988258 , -0.60385157, ...,  2.301635  ,\n",
       "           3.24841391, -2.87389977],\n",
       "         ...,\n",
       "         [-4.45949528, -4.29144445, -0.86631839, ..., -4.35422766,\n",
       "          -6.66941967,  5.62526936],\n",
       "         [ 1.978236  , -0.87962373, -1.94830525, ...,  0.66591269,\n",
       "          -2.08486619, -0.65062412],\n",
       "         [ 1.20331361,  0.8815082 ,  0.7549232 , ...,  0.63086029,\n",
       "           1.6418894 , -2.72041687]]],\n",
       "\n",
       "\n",
       "       [[[-0.97514989, -2.7806856 , -1.66661832, ...,  3.3015066 ,\n",
       "          -6.15041323,  3.29650219],\n",
       "         [-6.46808734,  2.1180065 , -5.03737706, ..., -6.69828342,\n",
       "           3.55053612,  0.06271979],\n",
       "         [ 5.91883256,  4.8802003 , -2.9641278 , ...,  0.28584974,\n",
       "          -2.99878292,  3.69639339],\n",
       "         ...,\n",
       "         [-3.11096146,  0.84343094,  4.21591613, ...,  8.11506747,\n",
       "          -1.04469782,  1.62752945],\n",
       "         [-0.50061868, -2.16657398,  1.17968642, ..., -2.94871359,\n",
       "          -0.82286662, -2.91646053],\n",
       "         [ 2.56135326,  1.51857625,  1.37094745, ..., -1.43293239,\n",
       "          -0.19555487,  0.08761839]]],\n",
       "\n",
       "\n",
       "       [[[ 0.84182048,  1.03266302, -2.27104156, ...,  3.55766725,\n",
       "          -0.74411676,  3.29210264],\n",
       "         [ 7.35888885,  2.44233304,  1.24112482, ...,  1.13818402,\n",
       "           1.03220532, -0.259739  ],\n",
       "         [-0.35057456,  0.80973253,  1.10170965, ...,  1.96130587,\n",
       "          -4.16687677, -7.05868087],\n",
       "         ...,\n",
       "         [ 2.71433087,  0.67781434, -1.80797324, ...,  2.43872377,\n",
       "          -3.10524163, -1.66207827],\n",
       "         [ 2.35422081, -2.994335  ,  1.19642683, ..., -2.91378923,\n",
       "           1.1848762 , -3.1471114 ],\n",
       "         [ 5.85506205, -3.20224582, -0.22162741, ..., -1.06786327,\n",
       "           4.51463829,  4.37672255]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvDenoiser(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(8, 8, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(8, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv3): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvDenoiser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvDenoiser, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 32), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding = 1)  \n",
    "        # conv layer (depth from 32 --> 16), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3, padding = 1)\n",
    "        # conv layer (depth from 16 --> 8), 3x3 kernels\n",
    "        self.conv3 = nn.Conv2d(16, 8, 3, padding = 1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        # transpose layer, a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(8, 8, 2, stride = 2)  # kernel_size=3 to get to a 7x7 image output\n",
    "        # two more transpose layers with a kernel of 2\n",
    "        self.t_conv2 = nn.ConvTranspose2d(8, 16, 2, stride = 2)\n",
    "        self.t_conv3 = nn.ConvTranspose2d(16, 32, 2, stride = 2)\n",
    "        # one, final, normal conv layer to decrease the depth\n",
    "        self.conv_out = nn.Conv2d(32, 1, 3, padding = 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # add third hidden layer\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "        # transpose again, output should have a sigmoid applied\n",
    "        x = F.sigmoid(self.conv_out(x))\n",
    "                \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvDenoiser()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Documents\\GitHub\\ConvHuberMC-Net\\Denoising_MC.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Documents/GitHub/ConvHuberMC-Net/Denoising_MC.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Visulase Model Layers\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Documents/GitHub/ConvHuberMC-Net/Denoising_MC.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m summary(model, input_size \u001b[39m=\u001b[39m [\u001b[39m40\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m160\u001b[39m, \u001b[39m320\u001b[39m], col_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39minput_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moutput_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnum_params\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtrainable\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summary' is not defined"
     ]
    }
   ],
   "source": [
    "# Visulase Model Layers\n",
    "summary(model, input_size = [40, 1, 160, 320], col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "We are only concerned with the training images, which we can get from the `train_loader`.\n",
    "\n",
    ">In this case, we are actually **adding some noise** to these images and we'll feed these `noisy_imgs` to our model. The model will produce reconstructed images based on the noisy input. But, we want it to produce _normal_ un-noisy images, and so, when we calculate the loss, we will still compare the reconstructed outputs to the original images!\n",
    "\n",
    "Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use `MSELoss`. And compare output images and input images as follows:\n",
    "```\n",
    "loss = criterion(outputs, images)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 10.306872\n",
      "Epoch: 2 \tTraining Loss: 10.106054\n",
      "Epoch: 3 \tTraining Loss: 10.082483\n",
      "Epoch: 4 \tTraining Loss: 10.143019\n",
      "Epoch: 5 \tTraining Loss: 10.013421\n",
      "Epoch: 6 \tTraining Loss: 10.104207\n",
      "Epoch: 7 \tTraining Loss: 9.892623\n",
      "Epoch: 8 \tTraining Loss: 9.870423\n",
      "Epoch: 9 \tTraining Loss: 9.963446\n",
      "Epoch: 10 \tTraining Loss: 10.199027\n",
      "Epoch: 11 \tTraining Loss: 10.302205\n",
      "Epoch: 12 \tTraining Loss: 10.135766\n",
      "Epoch: 13 \tTraining Loss: 10.231730\n",
      "Epoch: 14 \tTraining Loss: 10.261870\n",
      "Epoch: 15 \tTraining Loss: 10.267410\n",
      "Epoch: 16 \tTraining Loss: 9.963492\n",
      "Epoch: 17 \tTraining Loss: 9.980489\n",
      "Epoch: 18 \tTraining Loss: 10.028666\n",
      "Epoch: 19 \tTraining Loss: 9.886132\n",
      "Epoch: 20 \tTraining Loss: 9.755881\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# for adding noise to lowrank\n",
    "noise_factor=0.5\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for batch, (tensor) in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        # no need to flatten lowrank\n",
    "        lowrank, groundtruth = tensor[0], tensor[1]\n",
    "        lowrank, groundtruth = lowrank.unsqueeze(0), groundtruth.unsqueeze(0)\n",
    "        # print(lowrank.shape, groundtruth.shape)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        ## forward pass: compute predicted outputs by passing *noisy* lowrank to the model\n",
    "        outputs = model(lowrank)\n",
    "        # calculate the loss\n",
    "        # the \"target\" is still the original, not-noisy lowrank\n",
    "        loss = criterion(outputs, groundtruth)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*lowrank.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation/Inference Function\n",
    "\n",
    "# Move values to device\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module):\n",
    "\n",
    "    loss = 0\n",
    "    y_preds = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "      for batch, tensor in data_loader:\n",
    "        # Send data to the target device\n",
    "        lowrank, groundtruth = tensor[0], tensor[1]\n",
    "        lowrank, groundtruth = lowrank.unsqueeze(0), groundtruth.unsqueeze(0)\n",
    "        \n",
    "        outputs = model(lowrank)\n",
    "        # calculate the loss\n",
    "        # the \"target\" is still the original, not-noisy lowrank\n",
    "        loss = criterion(outputs, groundtruth)\n",
    "\n",
    "        # update running training loss\n",
    "        loss += loss.item() * lowrank.size(0)\n",
    "            \n",
    "      # print avg training statistics \n",
    "      loss = loss/len(data_loader)\n",
    "      print('Testing Loss: {:.6f}'.format(\n",
    "          loss\n",
    "          ))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss: 4.730606\n"
     ]
    }
   ],
   "source": [
    "eval_model(model = model, data_loader = test_loader, loss_fn = criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 160, 320]), torch.Size([5, 160, 320]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low, ground = next(iter(test_loader))\n",
    "low.shape, ground.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-15.1783, -14.8414, -14.4741,  ...,  16.2640,  18.4209,  20.2020])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(ground[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of sample 0 = 9.773709297180176\n",
      "Loss of sample 1 = 10.536747932434082\n",
      "Loss of sample 2 = 10.417193412780762\n",
      "Loss of sample 3 = 9.656516075134277\n",
      "Loss of sample 4 = 9.832962989807129\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    lowrank, groundtruth = low[i], ground[i]\n",
    "    lowrank, groundtruth = lowrank.unsqueeze(0), groundtruth.unsqueeze(0)\n",
    "\n",
    "    # Forward Pass\n",
    "    output = model.forward(lowrank)\n",
    "\n",
    "    # Loss\n",
    "    loss = criterion(outputs, groundtruth)\n",
    "\n",
    "    print(f'Loss of sample {i} = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "y_Omega_preds = []\n",
    "\n",
    "for i in range(len(M_test)):\n",
    "    y_preds.append(model(torch.tensor(M_test[i], dtype = torch.float)).cpu().detach().numpy())\n",
    "    y_Omega_preds.append(model(torch.tensor(M_Omega_test[i], dtype = torch.float)).cpu().detach().numpy())\n",
    "\n",
    "format_data.format(M_train, M_Omega_train, np.array(y_preds), np.array(y_Omega_preds), ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
