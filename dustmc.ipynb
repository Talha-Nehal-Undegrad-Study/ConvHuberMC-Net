{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Original saved in Tahir Sproj folder\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# File and System Interaction\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Date and Time Handling\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Neural Architecture\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    # %pip install torchinfo\n",
    "    from torchinfo import summary\n",
    "from scipy.fftpack import dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from python_scripts import dataset_processing\n",
    "from python_scripts import dustmc_training\n",
    "from python_scripts import logs_and_results\n",
    "# from python_scripts import img_pdf_compiler\n",
    "from python_scripts import utils\n",
    "from python_scripts import DUST_MC\n",
    "from python_scripts import dustmc_unrolled\n",
    "from python_scripts import test\n",
    "from python_scripts import generate_synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cpu',\n",
       " 'c:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/ConvHuberMC/DUSTMC_Data')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up some global variables\n",
    "\n",
    "ROOT = os.getcwd().replace('\\\\', '/') + '/DUSTMC_Data'\n",
    "# ROOT = 'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/ConvHuberMC/HuberMC_Data'\n",
    "# ROOT = 'C:/Users/HP/GitHub Workspace/ConvHuberMC-Net/HuberMC_Data'\n",
    "TRY = 'Try 1'\n",
    "SESSION = 'Session 1'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device, ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters --> for convhubermc:\n",
    "def get_default_param(gpu = True):\n",
    "    params_net = {}\n",
    "    params_net['size1'] = 150\n",
    "    params_net['size2'] = 300\n",
    "    params_net['rank'] = 10\n",
    "    \n",
    "    params_net['device'] = device\n",
    "        \n",
    "    params_net['layers'] = 3\n",
    "    params_net['CalInGPU'] = gpu\n",
    "    return params_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_net = dustmc_training.get_hyperparameter_grid('DUSTMC-Net', TrainInstances = 20, ValInstances = 10, BatchSize = 5, \n",
    "                                                          ValBatchSize = 2, num_epochs = 1, learning_rate = 0.001,\n",
    "                                                          K = 5, mu = 0, sigma = 1, m = 50, n = 150, d = 512, T = 300)\n",
    "params_net = get_default_param(gpu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 22461.8145,  23414.4824, -10364.0479,  ...,  13658.8359,\n",
      "         -25839.3691, -14172.5635],\n",
      "        [-24167.6016, -19116.0371, -17387.8379,  ...,  22746.5684,\n",
      "           5944.5254,   7228.3179],\n",
      "        [  8392.8857,  21603.7246,   8652.9229,  ...,  -3653.9675,\n",
      "         -16503.7324, -33827.3789],\n",
      "        ...,\n",
      "        [  3412.2625,  -2971.3630,   8200.4111,  ...,  10271.8154,\n",
      "          11220.6533,   -235.9009],\n",
      "        [ 15053.6738, -15064.4355,   8568.9912,  ...,  10210.1025,\n",
      "          -9404.7451,   6922.8564],\n",
      "        [   749.2056,   6144.6719,  19845.1680,  ...,  24413.5684,\n",
      "          13400.6416,   4954.7021]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "model = dustmc_unrolled.DustNet(hyper_param_net)\n",
    "S = torch.randn(hyper_param_net['n'], hyper_param_net['T'])\n",
    "output = model(S)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "DustNet                                  [150, 300]                37,502\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─BlueBoxLayer: 2-1                 [200, 300]                --\n",
       "│    │    └─SelfAttention: 3-1           [200, 300]                --\n",
       "│    │    └─LISTA: 3-2                   [200, 300]                50,001\n",
       "│    └─BlueBoxLayer: 2-2                 [200, 300]                --\n",
       "│    │    └─SelfAttention: 3-3           [200, 300]                --\n",
       "│    │    └─LISTA: 3-4                   [200, 300]                50,001\n",
       "│    └─BlueBoxLayer: 2-3                 [200, 300]                --\n",
       "│    │    └─SelfAttention: 3-5           [200, 300]                --\n",
       "│    │    └─LISTA: 3-6                   [200, 300]                50,001\n",
       "│    └─BlueBoxLayer: 2-4                 [200, 300]                --\n",
       "│    │    └─SelfAttention: 3-7           [200, 300]                --\n",
       "│    │    └─LISTA: 3-8                   [200, 300]                50,001\n",
       "│    └─BlueBoxLayer: 2-5                 [200, 300]                --\n",
       "│    │    └─SelfAttention: 3-9           [200, 300]                --\n",
       "│    │    └─LISTA: 3-10                  [200, 300]                50,001\n",
       "==========================================================================================\n",
       "Total params: 287,507\n",
       "Trainable params: 287,507\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.18\n",
       "Forward/backward pass size (MB): 2.40\n",
       "Params size (MB): 1.00\n",
       "Estimated Total Size (MB): 3.58\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size = [hyper_param_net['n'], hyper_param_net['T']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Name: Try 1 DUSTMC-Net mu 20.0% sigma 3.0\n",
      "\n",
      "Configuring Network...\n",
      "Instantiating Model...\n",
      "Model Instantiated...\n",
      "\n",
      "Parameters = \n",
      "{'size1': 150, 'size2': 300, 'rank': 10, 'device': 'cpu', 'layers': 3, 'CalInGPU': True}\n",
      "\n",
      "Epoch: 1, 2024-08-09 03:53:13, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some settings for visualisation\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set parameters (including hyperparameters) and setting for saving/logging data\n",
    "hyper_param_net = dustmc_training.get_hyperparameter_grid('DUSTMC-Net', TrainInstances = 20, ValInstances = 10, BatchSize = 5, \n",
    "                                                          ValBatchSize = 2, num_epochs = 20, learning_rate = 0.001,\n",
    "                                                          K = 5, mu = 0, sigma = 1, m = 50, n = 150, d = 200, T = 300)\n",
    "params_net = get_default_param(gpu = True)\n",
    "CalInGPU = params_net['CalInGPU']\n",
    "\n",
    "mu_list = [0.2]#, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "sigma_list = [3.0]#, 5.0, 6.0, 9.0]\n",
    "\n",
    "for mu in mu_list:\n",
    "    for sigma in sigma_list:\n",
    "        # ProjectName = TRY + ' ' + logs_and_results.get_current_time() + ' ' + hyper_param_net['Model'] + ' ' + 'Sampling Rate: ' + logs_and_results.get_q_str(q) + ' and DB ' + logs_and_results.get_noise_str(db)\n",
    "\n",
    "        ProjectName = TRY + ' ' + hyper_param_net['Model'] + ' mu ' + logs_and_results.get_q_str(mu) + ' sigma ' + logs_and_results.get_noise_str(sigma)\n",
    "        # Note: Removed time stamp from log file name as : not supported. Weird because this was not a problem in linux\n",
    "\n",
    "        # Get log file\n",
    "        logfile = logs_and_results.get_modularized_record(ProjectName, mu, sigma, 'Logs', hyper_param_net, params_net, ROOT, SESSION)\n",
    "        with open(logfile, 'w', 1) as log:\n",
    "            print('Project Name: %s\\n'%ProjectName)\n",
    "            log.write('Project Name: %s\\n\\n'%ProjectName)\n",
    "\n",
    "            # Get Model\n",
    "            net = dustmc_training.get_model(params_net, hyper_param_net, log)\n",
    "            print('\\nParameters = \\n%s\\n'%str(params_net))\n",
    "            log.write('\\nParameters = \\n%s\\n\\n'%str(params_net))\n",
    "\n",
    "            #Loading data and creating dataloader for both test and training\n",
    "            # print('Loading Data phase...')\n",
    "            log.write('Loading phase...\\n')\n",
    "            shape_dset = (params_net['size1'], params_net['size2'])\n",
    "            \n",
    "            train_loader, val_loader = dataset_processing.get_dataloaders(params_net = params_net, hyper_param_net = hyper_param_net, ROOT = ROOT)\n",
    "\n",
    "            # print('Finished loading.\\n')\n",
    "            log.write('Finished loading.\\n\\n')\n",
    "\n",
    "            # Some additional settings for training including loss, optimizer,\n",
    "            # floss = nn.functional.mse_loss(reduction = 'sum')\n",
    "            floss = nn.MSELoss()\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr = hyper_param_net['Lr'])\n",
    "            # scheduler2 =  torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma = 0.97, verbose = True)\n",
    "\n",
    "            # Array for recording parameter values after each layer for each epoch etc\n",
    "            outputs_L = dustmc_unrolled.to_var(torch.zeros([shape_dset[0], shape_dset[1]]), CalInGPU) \n",
    "            lossmean_vec = np.zeros((hyper_param_net['Epochs'], ))\n",
    "            lossmean_val_vec = np.zeros((hyper_param_net['Epochs'], ))\n",
    "\n",
    "\n",
    "            # dummy variable to monitor and record progress for loss\n",
    "            minloss = np.inf\n",
    "\n",
    "            for epoch in range(hyper_param_net['Epochs']):\n",
    "                print(f'Epoch: {epoch + 1}, {logs_and_results.get_current_time()}, \\n')\n",
    "                log.write(f'Epoch: {epoch + 1} ')\n",
    "                log.write(logs_and_results.get_current_time() + '\\n\\n')\n",
    "\n",
    "                # Train and Test Steps. (Record every 5 epochs)\n",
    "                if (epoch + 1) % 5 == 0:\n",
    "                    # print('Loading and calculating training batches...')\n",
    "                    log.write('Loading and calculating training batches...\\n')\n",
    "                    startime = time.time()\n",
    "                    loss_mean = dustmc_training.train_step(net, train_loader, floss, optimizer, hyper_param_net['TrainInstances'], hyper_param_net['BatchSize']) # remove alpha from train func\n",
    "                    endtime = time.time()\n",
    "                    # print('Training time is %f'%(endtime - startime))\n",
    "                    log.write('Training time is %f'%(endtime - startime))\n",
    "\n",
    "                    # print('Loading and calculating validation batches...')\n",
    "                    log.write('Loading and calculating validation batches...\\n')\n",
    "                    startime = time.time()\n",
    "                    loss_val_mean = dustmc_training.test_step(net, val_loader, floss, hyper_param_net['ValInstances'], hyper_param_net['ValBatchSize'])\n",
    "                    endtime = time.time()\n",
    "                    # print('Test time is %f'%(endtime - startime))\n",
    "                    log.write('Test time is %f'%(endtime - startime))\n",
    "\n",
    "                else:\n",
    "                    loss_mean = dustmc_training.train_step(net, train_loader, floss, optimizer, hyper_param_net['TrainInstances'], hyper_param_net['BatchSize'])\n",
    "                    loss_val_mean = dustmc_training.test_step(net, val_loader, floss, hyper_param_net['ValInstances'], hyper_param_net['ValBatchSize'])\n",
    "\n",
    "                # Update Record and Parameters\n",
    "                lossmean_vec[epoch] = loss_mean\n",
    "                lossmean_val_vec[epoch] = loss_val_mean\n",
    "\n",
    "\n",
    "                print('Epoch [%d/%d], Mean Training Loss:%.5e, Mean Validation Loss:%.5e'\n",
    "                      %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n",
    "\n",
    "                # Update Log after every 5 epochs. Make a plot of MSE against epochs every 5 epochs. Save Model in whole/dict form every five epochs.\n",
    "                if (epoch + 1) % 5 == 0:\n",
    "                    print(f\"Saving Whole Model at Epochs: [{epoch + 1}/{hyper_param_net['Epochs']}]\")\n",
    "                    model_whole_path = logs_and_results.get_modularized_record(ProjectName, mu, sigma, 'Saved Models - Whole', hyper_param_net, params_net, ROOT, SESSION, current_epoch = epoch + 1)\n",
    "                    # torch.save(net, model_whole_path)\n",
    "                    print(f\"Saving Model Dict at Epochs: [{epoch + 1}/{hyper_param_net['Epochs']}]\")\n",
    "                    model_state_dict_path = logs_and_results.get_modularized_record(ProjectName, mu, sigma, 'Saved Models - Dict', hyper_param_net, params_net, ROOT, SESSION, current_epoch = epoch + 1)\n",
    "                    # torch.save(net.state_dict(), model_state_dict_path)\n",
    "\n",
    "                    log.write('Epoch [%d/%d], Mean Training Loss:%.5e, Mean Validation Loss:%.5e\\n'\n",
    "                              %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n",
    "                    np.set_printoptions(precision = 3)\n",
    "\n",
    "                    if True or loss_val_mean < minloss:\n",
    "                        # print('saved at [epoch%d/%d]'%(epoch + 1, hyper_param_net['Epochs']))\n",
    "                        log.write('saved at [epoch%d/%d]\\n' %(epoch + 1, hyper_param_net['Epochs']))\n",
    "                        minloss = min(loss_val_mean, minloss)\n",
    "\n",
    "            # Finish off by observing the minimum loss on validation set\n",
    "\n",
    "            #Print min loss\n",
    "            # print('\\nMin Loss = %.4e'%np.min(lossmean_val_vec))\n",
    "            log.write('\\nMin Loss = %.4e'%np.min(lossmean_val_vec))\n",
    "\n",
    "            # Plotting MSE vs Epoch and Saving it\n",
    "\n",
    "            # Get Directory where we have to save the plot\n",
    "            dir = logs_and_results.get_modularized_record(ProjectName, mu, sigma, 'Plots', hyper_param_net, params_net, ROOT, SESSION, current_epoch = epoch + 1)\n",
    "            logs_and_results.plot_and_save_mse_vs_epoch(lossmean_vec, lossmean_val_vec, dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_i: tensor([0.0221, 0.0345, 0.0003, 0.0150, 0.0002])\n",
      "Indexed value: 0.0003437580307945609\n",
      "Type of indexed value: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example values\n",
    "DH = torch.randn(10, 5)  # Shape (n, T)\n",
    "\n",
    "# Compute βi values\n",
    "beta_i = torch.exp(-0.5 * torch.sum(DH ** 2, dim=0))  # Shape (T,)\n",
    "\n",
    "# Ensure indexing returns a tensor\n",
    "index = 2\n",
    "indexed_value = beta_i[index]\n",
    "\n",
    "print(f\"beta_i: {beta_i}\")\n",
    "print(f\"Indexed value: {indexed_value}\")\n",
    "print(f\"Type of indexed value: {type(indexed_value)}\")  # Should be <class 'torch.Tensor'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant arguements for DUSTMC\n",
    "\n",
    "M_train, M_Omega_train, M_test, M_Omega_test = generate_synthetic_data.generate_simple_gaussian_noise(150, 300, 10, 10, 2, 0.5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "rand_batch_idx = np.random.randint(0, M_Omega_train.shape[0])\n",
    "X = M_Omega_train[rand_batch_idx, :, :]\n",
    "\n",
    "# Initialize a m x n A measurement sensing matrix with random values from a uniform distribution between 0 and 1\n",
    "A = np.random.uniform(0, 1, (100, 150)) # A is m x n. Since m << n and n is normally 150\n",
    "\n",
    "# Initalize the Dictonary D which is n x d and d >> n by DCT. d is taken as 512\n",
    "random_matrix = np.random.uniform(0, 1, (150, 512))\n",
    "\n",
    "# Apply the Discrete Cosine Transform (DCT) to the matrix\n",
    "D = dct(random_matrix, type = 2, norm = 'ortho')\n",
    "\n",
    "# Initialize K (no. of iterations), and hyperparameters lambda_1, lambda_2, and c\n",
    "K = 50\n",
    "lambda_1 = 0.1\n",
    "lambda_2 = 0.4\n",
    "c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nehal/GitHubWorkspace/ConvHuberMC-Net/python_scripts/test.py:49: RuntimeWarning: overflow encountered in exp\n",
      "  exp_values = np.exp(exponent_terms)\n",
      "/home/nehal/GitHubWorkspace/ConvHuberMC-Net/python_scripts/test.py:52: RuntimeWarning: invalid value encountered in multiply\n",
      "  G_numerator = np.sum(beta_values[:, None] * exp_values[:, None] * H.T, axis=0)\n",
      "/home/nehal/GitHubWorkspace/ConvHuberMC-Net/python_scripts/test.py:53: RuntimeWarning: invalid value encountered in multiply\n",
      "  G_denominator = np.sum(beta_values * exp_values)\n"
     ]
    }
   ],
   "source": [
    "# Now run the attention based algo on X to get its recovered/reconstructed form S\n",
    "S = test.attention_based_algo(A, D, X, K, lambda_1, lambda_2, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The squared L2 norm loss is: nan\n"
     ]
    }
   ],
   "source": [
    "# Now measure loss between reconstructed and groundtruth\n",
    "\n",
    "groundtruth = M_train[rand_batch_idx, :, :]\n",
    "\n",
    "print(f'The squared L2 norm loss is: {utils.compute_squared_l2_norm_loss(S, groundtruth)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
