{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33035,"status":"ok","timestamp":1696836126900,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"QkewbYGcurVf","outputId":"10da4d48-8e12-4001-d652-864b9e977ded"},"outputs":[],"source":["# Deep Learning Libraries\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.utils.data as data\n","\n","# Data Manipulation and Analysis\n","import numpy as np\n","import pandas as pd\n","import collections # A module providing alternative data structures like named tuples, defaultdict, Counter, etc., compared to built-in Python containers.\n","import random\n","\n","# Data Visualization\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn\n","\n","# File and System Interaction\n","import glob\n","import os\n","from pathlib import Path\n","import shutil\n","\n","# Scientific Computing and Math\n","import math\n","import cmath\n","\n","# Date and Time Handling\n","import time\n","import datetime\n","\n","# Linear Algebra\n","from torch import linalg as LA\n","\n","# Neural Architecture\n","try:\n","    from torchinfo import summary\n","except:\n","    !pip install torchinfo\n","    from torchinfo import summary"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# %load_ext autoreload\n","# %autoreload 2\n","%reload_ext autoreload\n","\n","# from python_scripts import dataset_processing\n","# from python_scripts import architecture\n","# from python_scripts import training\n","# from python_scripts import logs_and_results\n","from python_scripts import lp1_reg, utils"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import numpy as np\n","import scipy.io"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["mat_data1 = scipy.io.loadmat('M_Omega_train1.mat')\n","mat_data2 = scipy.io.loadmat('M_Omega_test1.mat')\n","mat_data3 = scipy.io.loadmat('M_train1.mat')\n","mat_data4 = scipy.io.loadmat('M_test1.mat')\n","\n","m1 = mat_data1['M_Omega_train'][:, :, 0]\n","m2 = mat_data2['M_Omega_test'][:, :, 0]\n","m3 = mat_data3['M_train'][:, :, 0]\n","m4 = mat_data4['M_test'][:, :, 0]\n","\n","Omega = utils.get_nonzeros(m1)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["lp1_reg.LP1(m1, Omega, 10, 500)"]},{"cell_type":"markdown","metadata":{"id":"24abWA7yyU5Q"},"source":["#### DataSet_Unfolded for Real World Sensing Data.py"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1696836126902,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"zwYsPMqG68M6"},"outputs":[],"source":["# Setting up some global variables\n","\n","ROOT = 'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/ConvHuberMC/HuberMC_Data'\n","TRY = '1st try'\n","SESSION = 'Session 1'"]},{"cell_type":"markdown","metadata":{"id":"reS6gl1b1M70"},"source":["#### Model path of loading"]},{"cell_type":"markdown","metadata":{"id":"RGsjtUu7-iJ5"},"source":["Testing Training Loop"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["'cpu'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1696836126902,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"nOIBCr5gIr8Y"},"outputs":[],"source":["# Get parameters --> for convhubermc: c, lambda, sigma, mu, delta, tau\n","def get_default_param(gpu = True):\n","    params_net = {}\n","    params_net['size1'] = 150\n","    params_net['size2'] = 300\n","    params_net['rank'] = 10\n","    \n","    params_net['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    params_net['hubreg_iters'] = 2\n","    params_net['layers'] = 3\n","    params_net['initial_sigma'] = 0 # may not be learnable as then tau and lamda no longer needed\n","    params_net['initial_c'] = 1.345\n","    params_net['initial_lamda'] = 1\n","    params_net['initial_mu'] = 0\n","    params_net['CalInGPU'] = gpu\n","    \n","    return params_net"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# seed = 123\n","# torch.manual_seed(seed)\n","# target = (torch.randn(30, 50))\n","# input_tensor = target * torch.bernoulli(torch.full((30, 50), 0.2))\n","# model = architecture.UnfoldedNet_Huber(params = get_default_param(False))\n","\n","# criterion = nn.MSELoss()\n","# optimizer = torch.optim.Adam(model.parameters())\n","\n","# model.train()\n","# output = model(input_tensor)\n","\n","# loss = (criterion(output, target))/torch.square(torch.norm(target, p = 'fro'))\n","# optimizer.zero_grad()\n","# print(f'loss before backward: {loss}, loss.grad: {loss.requires_grad}')\n","# loss.backward()\n","# print(f'loss: {loss}')\n","# print(\"\\nGradients after one epoch:\")\n","# for name, param in model.named_parameters():\n","#     print(f'name: {name}\\t\\tgradient: {param.grad}')\n","# optimizer.step()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# # Create a input_tensor of random indices\n","# shuffled_indices = torch.randperm(input_tensor.nelement())\n","\n","# # Index the original input_tensor with these shuffled indices\n","# shuffled_input_tensor = input_tensor.view(-1)[shuffled_indices].view(input_tensor.size())\n","\n","# output = model(shuffled_input_tensor)\n","\n","# loss = (criterion(output, target))/torch.square(torch.norm(target, p = 'fro'))\n","# optimizer.zero_grad()\n","# print(f'loss before backward: {loss}, loss.grad: {loss.requires_grad}')\n","# loss.backward()\n","# print(f'loss: {loss}')\n","# print(\"\\nGradients after one epoch:\")\n","# for name, param in model.named_parameters():\n","#     print(f'name: {name}\\t\\tgradient: {param.grad}')\n","# optimizer.step()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# # Create a input_tensor of random indices\n","# shuffled_indices = torch.randperm(input_tensor.nelement())\n","\n","# # Index the original input_tensor with these shuffled indices\n","# shuffled_input_tensor = input_tensor.view(-1)[shuffled_indices].view(input_tensor.size())\n","\n","# output = model(shuffled_input_tensor)\n","\n","# loss = (criterion(output, target))/torch.square(torch.norm(target, p = 'fro'))\n","# optimizer.zero_grad()\n","# print(f'loss before backward: {loss}, loss.grad: {loss.requires_grad}')\n","# loss.backward()\n","# print(f'loss: {loss}')\n","# print(\"\\nGradients after one epoch:\")\n","# for name, param in model.named_parameters():\n","#     print(f'name: {name}\\t\\tgradient: {param.grad}')\n","# optimizer.step()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["UnfoldedNet_Huber(\n","  (huber_obj): Sequential(\n","    (0): Huber()\n","    (1): Huber()\n","    (2): Huber()\n","  )\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model = architecture.UnfoldedNet_Huber(params = get_default_param(False))\n","model"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# model.forward(torch.randn(30, 50) * torch.bernoulli(torch.full((30, 50), 0.2)))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# summary(model, input_size = [30, 50])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":653},"executionInfo":{"elapsed":415,"status":"error","timestamp":1696360284646,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"bmMuNqIs97hm","outputId":"ebed6b71-2dc9-42fe-b8e7-399ecf985c27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Project Name: 1st try HuberMC-Net Sampling Rate: 20.0% and DB 3.0\n","Configuring Network...\n","\n","Instantiating Model...\n","\n","Model Instantiated...\n","\n","Parameters = \n","{'size1': 150, 'size2': 300, 'rank': 10, 'device': 'cpu', 'hubreg_iters': 2, 'layers': 3, 'initial_sigma': 0.67, 'initial_c': 1.345, 'initial_lamda': 1, 'initial_mu': 0, 'CalInGPU': False}\n","\n","Loading Data phase...\n","----------------\n"]},{"name":"stdout","output_type":"stream","text":["Finished loading.\n","\n","Epoch: 1, 2024-02-08 20:53:43, \n","\n","Epoch [1/20], Lossmean:4.21930e-05, Validation lossmean:6.38541e+00\n","c: [1.347388505935669, 1.3473883867263794, 1.347388505935669], lamda: [1.0022563934326172, 1.002255916595459, 1.0022560358047485], mu: [0.003965074196457863, 0.0039650737307965755, 0.003965072799474001]\n","Epoch: 2, 2024-02-08 21:25:50, \n","\n"]}],"source":["# Some settings for visualisation\n","matplotlib.use('Agg')\n","%matplotlib inline\n","\n","seed = 123\n","torch.manual_seed(seed)\n","\n","# Set parameters (including hyperparameters) and setting for saving/logging data\n","\n","hyper_param_net = training.get_hyperparameter_grid('HuberMC-Net', TrainInstances = 20, ValInstances = 10, BatchSize = 5, ValBatchSize = 2, num_epochs = 20, learning_rate = 0.001)\n","\n","params_net = get_default_param(gpu = False)\n","\n","CalInGPU = params_net['CalInGPU']\n","\n","q_list = [0.2]\n","db_list = [3.0]\n","\n","for q in q_list:\n","  for db in db_list:\n","    # ProjectName = TRY + ' ' + logs_and_results.get_current_time() + ' ' + hyper_param_net['Model'] + ' ' + 'Sampling Rate: ' + logs_and_results.get_q_str(q) + ' and DB ' + logs_and_results.get_noise_str(db)\n","    \n","    ProjectName = TRY + ' ' + hyper_param_net['Model'] + ' ' + 'Sampling Rate: ' + logs_and_results.get_q_str(q) + ' and DB ' + logs_and_results.get_noise_str(db)\n","    # Note: Removed time stamp from log file name as : not supported. Weird because this was not a problem in linux\n","    \n","    # Get log file\n","    logfile = logs_and_results.get_modularized_record(ProjectName, q, db, 'Logs', hyper_param_net, params_net, SESSION)\n","    log = open(logfile, 'w')\n","    print('Project Name: %s'%ProjectName)\n","    log.write('Project Name: %s\\n'%ProjectName)\n","\n","    # Get Model\n","    net = training.get_model(params_net, hyper_param_net, log)\n","    print('Parameters = \\n%s\\n'%str(params_net))\n","    log.write('params_net = \\n%s\\n\\n'%str(params_net))\n","\n","    #Loading data and creating dataloader for both test and training\n","    print('Loading Data phase...')\n","    print('----------------')\n","    log.write('Loading phase...\\n')\n","    log.write('----------------\\n')\n","    shape_dset = (params_net['size1'], params_net['size2'])\n","    \n","    train_loader, val_loader = dataset_processing.get_dataloaders(params_net = params_net, hyper_param_net = hyper_param_net, sampling_rate = q, db = db)\n","\n","    print('Finished loading.\\n')\n","    log.write('Finished loading.\\n\\n');\n","\n","    # Some additional settings for training including loss, optimizer,\n","    # floss = nn.functional.mse_loss(reduction = 'sum')\n","    floss = nn.MSELoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr = hyper_param_net['Lr'])\n","    # scheduler2 =  torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma = 0.97, verbose = True)\n","\n","    # Array for recording parameter values after each layer for each epoch etc\n","    outputs_L = architecture.to_var(torch.zeros([shape_dset[0], shape_dset[1]]), CalInGPU) \n","    lossmean_vec = np.zeros((hyper_param_net['Epochs'], ))\n","    lossmean_val_vec = np.zeros((hyper_param_net['Epochs'], ))\n","\n","    c_list, lamda_list, mu_list = net.getexp_LS()\n","\n","    c_list_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","    lamda_list_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","    mu_list_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","\n","    # dummy variable to monitor and record progress for loss\n","    minloss = np.inf\n","\n","    for epoch in range(hyper_param_net['Epochs']):\n","      print(f'Epoch: {epoch + 1}, {logs_and_results.get_current_time()}, \\n')\n","      log.write('\\n' + logs_and_results.get_current_time() + '\\n')\n","\n","      # Train and Test Steps. (Record every 5 epochs)\n","      if (epoch + 1) % 5 == 0:\n","          print('Loading and calculating training batches...')\n","          log.write('Loading and calculating training batches...\\n')\n","          startime = time.time()\n","          loss_mean = training.train_step(net, train_loader, floss, optimizer, CalInGPU, hyper_param_net['TrainInstances'], hyper_param_net['BatchSize']) # remove alpha from train func\n","          endtime = time.time()\n","          print('Training time is %f'%(endtime - startime))\n","          log.write('Training time is %f\\n'%(endtime - startime))\n","\n","          print('Loading and calculating validation batches...')\n","          log.write('Loading and calculating validation batches...\\n')\n","          startime = time.time()\n","          loss_val_mean = training.test_step(net, val_loader, floss, CalInGPU, hyper_param_net['ValInstances'], hyper_param_net['ValBatchSize'])\n","          endtime = time.time()\n","          print('Test time is %f'%(endtime - startime))\n","          log.write('Test time is %f\\n'%(endtime - startime))\n","\n","      else:\n","        loss_mean = training.train_step(net, train_loader, floss, optimizer, CalInGPU, hyper_param_net['TrainInstances'], hyper_param_net['BatchSize'])\n","        loss_val_mean = training.test_step(net, val_loader, floss, CalInGPU, hyper_param_net['ValInstances'], hyper_param_net['ValBatchSize'])\n","\n","      # Update Record and Parameters\n","      lossmean_vec[epoch] = loss_mean\n","      lossmean_val_vec[epoch] = loss_val_mean\n","\n","      c_list, lamda_list, mu_list = net.getexp_LS()\n","\n","      c_list_vec[epoch, :] = c_list\n","      lamda_list_vec[epoch, :] = lamda_list\n","      mu_list_vec[epoch, :] = mu_list\n","\n","      print('Epoch [%d/%d], Lossmean:%.5e, Validation lossmean:%.5e'\n","            %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n","      # print('loss_lowrank_mean', loss_lowrank_mean)\n","      # print('loss_val_lowrank_mean', loss_val_lowrank_mean)\n","      print(f'c: {c_list}, lamda: {lamda_list}, mu: {mu_list}')\n","\n","      # Update Log after every 5 epochs. Make a plot of MSE against epochs every 5 epochs. Save Model in whole/dict form every five epochs.\n","      if (epoch + 1) % 5 == 0:\n","        print(f\"Saving Whole Model at Epochs: [{epoch + 1}/{hyper_param_net['Epochs']}]\")\n","        model_whole_path = logs_and_results.get_modularized_record(ProjectName, q, db, 'Saved Models - Whole', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        # torch.save(net, model_whole_path)\n","        print(f\"Saving Model Dict at Epochs: [{epoch + 1}/{hyper_param_net['Epochs']}]\")\n","        model_state_dict_path = logs_and_results.get_modularized_record(ProjectName, q, db, 'Saved Models - Dict', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        # torch.save(net.state_dict(), model_state_dict_path)\n","\n","        # print('Epoch [%d/%d], Lossmean:%.5e, Validation lossmean:%.5e'\n","        # %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n","        # print('loss_lowrank_mean', loss_lowrank_mean)\n","        # print('loss_val_lowrank_mean', loss_val_lowrank_mean)\n","        # print(f'c: {c_list}, lamda: {lamda_list}, mu: {mu_list}')\n","\n","        # log.write('loss_lowrank_mean %.5e\\n' %(loss_lowrank_mean))\n","        # log.write('loss_val_lowrank_mean %.5e\\n' %(loss_val_lowrank_mean))\n","        log.write('Epoch [%d/%d], Lossmean:%.5e, Validation lossmean:%.5e\\n'\n","              %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n","        np.set_printoptions(precision = 3)\n","\n","        log.write('c_list: '+ str(c_list)+'\\n')\n","        log.write('lamda_list: '+ str(lamda_list)+'\\n')\n","        log.write('mu_list: '+ str(mu_list)+'\\n')\n","\n","        if True or loss_val_mean < minloss:\n","          print('saved at [epoch%d/%d]'%(epoch + 1, hyper_param_net['Epochs']))\n","          log.write('saved at [epoch%d/%d]\\n' %(epoch + 1, hyper_param_net['Epochs']))\n","          minloss = min(loss_val_mean, minloss)\n","\n","        # Plotting MSE vs Epoch and Saving it\n","\n","        # Get Directory where we have to save the plot\n","        dir = logs_and_results.get_modularized_record(ProjectName, q, db, 'Plots', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        epochs_vec = np.arange(0, hyper_param_net['Epochs'], 1)\n","        logs_and_results.plot_and_save_mse_vs_epoch(epochs_vec, lossmean_vec, hyper_param_net, lossmean_val_vec, dir, epoch)\n","\n","    # Finish off by observing the minimum loss on validation set\n","\n","    #Print min loss\n","    print('\\nmin Loss = %.4e'%np.min(lossmean_val_vec))\n","    log.write('\\nmin Loss = %.4e\\n'%np.min(lossmean_val_vec))\n","    log.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
