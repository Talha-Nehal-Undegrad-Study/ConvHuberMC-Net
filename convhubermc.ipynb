{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33035,"status":"ok","timestamp":1696836126900,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"QkewbYGcurVf","outputId":"10da4d48-8e12-4001-d652-864b9e977ded"},"outputs":[],"source":["# Deep Learning Libraries\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.utils.data as data\n","\n","# Data Manipulation and Analysis\n","import numpy as np\n","import pandas as pd\n","import collections # A module providing alternative data structures like named tuples, defaultdict, Counter, etc., compared to built-in Python containers.\n","import random\n","\n","# Data Visualization\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn\n","\n","# File and System Interaction\n","import glob\n","import os\n","from pathlib import Path\n","import shutil\n","\n","# Scientific Computing and Math\n","import math\n","import cmath\n","\n","# Date and Time Handling\n","import time\n","import datetime\n","\n","# Linear Algebra\n","from torch import linalg as LA\n","\n","# Neural Architecture\n","try:\n","    from torchinfo import summary\n","except:\n","    !pip install torchinfo\n","    from torchinfo import summary"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# %load_ext autoreload\n","# %autoreload 2\n","%reload_ext autoreload\n","\n","from python_scripts import dataset_processing\n","from python_scripts import architecture\n","from python_scripts import training\n","from python_scripts import logs_and_results"]},{"cell_type":"markdown","metadata":{"id":"24abWA7yyU5Q"},"source":["#### DataSet_Unfolded for Real World Sensing Data.py"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1696836126902,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"zwYsPMqG68M6"},"outputs":[],"source":["# Setting up some global variables\n","\n","ROOT = 'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/ConvHuberMC/HuberMC_Data'\n","TRY = '1st try'\n","SESSION = 'Session 1'"]},{"cell_type":"markdown","metadata":{"id":"reS6gl1b1M70"},"source":["#### Model path of loading"]},{"cell_type":"markdown","metadata":{"id":"RGsjtUu7-iJ5"},"source":["Testing Training Loop"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["'cpu'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1696836126902,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"nOIBCr5gIr8Y"},"outputs":[],"source":["# Get parameters --> for convhubermc: c, lambda, sigma, mu, delta, tau\n","def get_default_param(gpu = True):\n","    params_net = {}\n","    params_net['size1'] = 30\n","    params_net['size2'] = 50\n","    params_net['rank'] = 5\n","    \n","    params_net['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    params_net['hubreg_iters'] = 1\n","    params_net['layers'] = 3\n","    params_net['initial_sigma'] = 0.67 # may not be learnable as then tau and lamda no longer needed\n","    params_net['initial_c'] = 1.345\n","    params_net['initial_lamda'] = 1\n","    params_net['initial_mu'] = 0\n","    params_net['CalInGPU'] = gpu\n","    \n","    return params_net"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# seed = 123\n","# torch.manual_seed(seed)\n","# target = (torch.randn(30, 50))\n","# input_tensor = target * torch.bernoulli(torch.full((30, 50), 0.2))\n","# model = architecture.UnfoldedNet_Huber(params = get_default_param(False))\n","\n","# criterion = nn.MSELoss()\n","# optimizer = torch.optim.Adam(model.parameters())\n","\n","# model.train()\n","# output = model(input_tensor)\n","\n","# loss = (criterion(output, target))/torch.square(torch.norm(target, p = 'fro'))\n","# optimizer.zero_grad()\n","# print(f'loss before backward: {loss}, loss.grad: {loss.requires_grad}')\n","# loss.backward()\n","# print(f'loss: {loss}')\n","# print(\"\\nGradients after one epoch:\")\n","# for name, param in model.named_parameters():\n","#     print(f'name: {name}\\t\\tgradient: {param.grad}')\n","# optimizer.step()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# # Create a input_tensor of random indices\n","# shuffled_indices = torch.randperm(input_tensor.nelement())\n","\n","# # Index the original input_tensor with these shuffled indices\n","# shuffled_input_tensor = input_tensor.view(-1)[shuffled_indices].view(input_tensor.size())\n","\n","# output = model(shuffled_input_tensor)\n","\n","# loss = (criterion(output, target))/torch.square(torch.norm(target, p = 'fro'))\n","# optimizer.zero_grad()\n","# print(f'loss before backward: {loss}, loss.grad: {loss.requires_grad}')\n","# loss.backward()\n","# print(f'loss: {loss}')\n","# print(\"\\nGradients after one epoch:\")\n","# for name, param in model.named_parameters():\n","#     print(f'name: {name}\\t\\tgradient: {param.grad}')\n","# optimizer.step()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# # Create a input_tensor of random indices\n","# shuffled_indices = torch.randperm(input_tensor.nelement())\n","\n","# # Index the original input_tensor with these shuffled indices\n","# shuffled_input_tensor = input_tensor.view(-1)[shuffled_indices].view(input_tensor.size())\n","\n","# output = model(shuffled_input_tensor)\n","\n","# loss = (criterion(output, target))/torch.square(torch.norm(target, p = 'fro'))\n","# optimizer.zero_grad()\n","# print(f'loss before backward: {loss}, loss.grad: {loss.requires_grad}')\n","# loss.backward()\n","# print(f'loss: {loss}')\n","# print(\"\\nGradients after one epoch:\")\n","# for name, param in model.named_parameters():\n","#     print(f'name: {name}\\t\\tgradient: {param.grad}')\n","# optimizer.step()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["UnfoldedNet_Huber(\n","  (huber_obj): Sequential(\n","    (0): Huber()\n","    (1): Huber()\n","    (2): Huber()\n","  )\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model = architecture.UnfoldedNet_Huber(params = get_default_param(False))\n","model"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# model.forward(torch.randn(30, 50) * torch.bernoulli(torch.full((30, 50), 0.2)))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# summary(model, input_size = [30, 50])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":653},"executionInfo":{"elapsed":415,"status":"error","timestamp":1696360284646,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"bmMuNqIs97hm","outputId":"ebed6b71-2dc9-42fe-b8e7-399ecf985c27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Project Name: 1st try HuberMC-Net Sampling Rate: 20.0% and DB 3.0\n","Configuring Network...\n","\n","Instantiating Model...\n","\n","Model Instantiated...\n","\n","Parameters = \n","{'size1': 30, 'size2': 50, 'rank': 5, 'device': 'cpu', 'hubreg_iters': 1, 'layers': 3, 'initial_sigma': 0.67, 'initial_c': 1.345, 'initial_lamda': 1, 'initial_mu': 0, 'CalInGPU': False}\n","\n","Loading Data phase...\n","----------------\n","Finished loading.\n","\n","Epoch: 1, 2024-01-30 18:47:58, \n","\n","Epoch [1/5], Lossmean:1.74933e+01, Validation lossmean:1.48754e+01\n","c: [1.3417689800262451, 1.3413398265838623, 1.3408483266830444], lamda: [0.9991861581802368, 0.9975610971450806, 0.9950698614120483], mu: [-0.0015641035279259086, -0.000672783819027245, -0.00042173388646915555]\n","Epoch: 2, 2024-01-30 18:51:01, \n","\n","Epoch [2/5], Lossmean:1.90630e+01, Validation lossmean:1.65249e+01\n","c: [1.34085214138031, 1.3403576612472534, 1.3391722440719604], lamda: [0.99844890832901, 0.995151698589325, 0.9907755851745605], mu: [-0.0022362051531672478, -0.0007695042295381427, -0.0006306324503384531]\n","Epoch: 3, 2024-01-30 18:54:07, \n","\n","Epoch [3/5], Lossmean:2.08080e+01, Validation lossmean:1.74934e+01\n","c: [1.3394343852996826, 1.3394731283187866, 1.337981104850769], lamda: [0.9939113855361938, 0.9917298555374146, 0.9869042038917542], mu: [-0.0012252710293978453, 0.0015622042119503021, 0.001632027910090983]\n","Epoch: 4, 2024-01-30 21:40:32, \n","\n","Epoch [4/5], Lossmean:1.99111e+01, Validation lossmean:1.51396e+01\n","c: [1.3381686210632324, 1.3413059711456299, 1.3408267498016357], lamda: [0.9894867539405823, 0.9970357418060303, 0.9928061366081238], mu: [0.000574363861232996, 0.004971669986844063, 0.004992370028048754]\n","Epoch: 5, 2024-01-30 21:42:30, \n","\n","Loading and calculating training batches...\n","Training time is 113.313426\n","Loading and calculating validation batches...\n","Test time is 0.753298\n","Epoch [5/5], Lossmean:1.65742e+01, Validation lossmean:1.10744e+01\n","c: [1.337965726852417, 1.3476104736328125, 1.3486300706863403], lamda: [0.9883680939674377, 1.0064692497253418, 1.0025829076766968], mu: [0.0025990272406488657, 0.008575500920414925, 0.008567845448851585]\n","Saving Whole Model at Epochs: [5/5]\n","Saving Model Dict at Epochs: [5/5]\n","saved at [epoch5/5]\n"]},{"ename":"ValueError","evalue":"x and y must have same first dimension, but have shapes (1,) and (0,)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[13], line 146\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m logs_and_results\u001b[38;5;241m.\u001b[39mget_modularized_record(ProjectName, q, db, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlots\u001b[39m\u001b[38;5;124m'\u001b[39m, hyper_param_net, params_net, SESSION, current_epoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    145\u001b[0m     epochs_vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, hyper_param_net[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 146\u001b[0m     \u001b[43mlogs_and_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_and_save_mse_vs_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossmean_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyper_param_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossmean_val_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Finish off by observing the minimum loss on validation set\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m#Print min loss\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mmin Loss = \u001b[39m\u001b[38;5;132;01m%.4e\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mnp\u001b[38;5;241m.\u001b[39mmin(lossmean_val_vec))\n","File \u001b[1;32mc:\\Users\\Talha\\OneDrive - Higher Education Commission\\Documents\\GitHub\\ConvHuberMC\\python_scripts\\logs_and_results.py:88\u001b[0m, in \u001b[0;36mplot_and_save_mse_vs_epoch\u001b[1;34m(epochs_vec, lossmean_vec, hyper_param_net, lossmean_val_vec, dir, current_epoch)\u001b[0m\n\u001b[0;32m     86\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m), dpi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     87\u001b[0m epochs_vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, hyper_param_net[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs_vec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossmean_vec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs_vec[current_epoch \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m5\u001b[39m: current_epoch], lossmean_val_vec[current_epoch \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m5\u001b[39m: current_epoch], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-*\u001b[39m\u001b[38;5;124m'\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_val\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     90\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n","File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Talha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axes\\_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (0,)"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAqoAAAH/CAYAAACfLv+zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfpUlEQVR4nO3df2zV9b348RetttXMVrxcyo9bx9Vd5zYVHEhXHTHe9K6Jhl3+uBlXF+ASp9eNaxzNvRP8QefcKNepIZk4ItPrkjsvbEa9yyB4Xe/I4uwNGdDEXUHj0MFd1gp3l5bh1kr7+f6x2H0rxXFqW17C45GcP/re+30+77O3bE8/PecwoSiKIgAAIJmyk70BAAAYjlAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAIKWSQ/XHP/5xzJ8/P6ZNmxYTJkyIZ5555o+u2bZtW3z84x+PysrK+NCHPhSPP/74CLYKAMDppORQPXLkSMycOTPWrVt3QvNfe+21uO666+Kaa66Jjo6O+OIXvxif+9zn4tlnny15swAAnD4mFEVRjHjxhAnx9NNPx4IFC4475/bbb4/NmzfHz372s8Gxv/3bv41Dhw7F1q1bR3ppAABOcWeM9QXa29ujsbFxyFhTU1N88YtfPO6a3t7e6O3tHfx5YGAgfv3rX8ef/MmfxIQJE8ZqqwAAjFBRFHH48OGYNm1alJWNzsegxjxUOzs7o7a2dshYbW1t9PT0xG9/+9s466yzjlnT2toa99xzz1hvDQCAUbZ///74sz/7s1F5rjEP1ZFYuXJlNDc3D/7c3d0d559/fuzfvz+qq6tP4s4AABhOT09P1NXVxTnnnDNqzznmoTplypTo6uoaMtbV1RXV1dXD3k2NiKisrIzKyspjxqurq4UqAEBio/k2zTH/HtWGhoZoa2sbMvbcc89FQ0PDWF8aAID3sZJD9Te/+U10dHRER0dHRPz+66c6Ojpi3759EfH7X9svXrx4cP4tt9wSe/fujS996UuxZ8+eePjhh+O73/1uLF++fHReAQAAp6SSQ/WnP/1pXH755XH55ZdHRERzc3NcfvnlsWrVqoiI+NWvfjUYrRERf/7nfx6bN2+O5557LmbOnBkPPPBAfOtb34qmpqZRegkAAJyK3tP3qI6Xnp6eqKmpie7ubu9RBQBIaCx6bczfowoAACMhVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApDSiUF23bl3MmDEjqqqqor6+PrZv3/6u89euXRsf/vCH46yzzoq6urpYvnx5/O53vxvRhgEAOD2UHKqbNm2K5ubmaGlpiZ07d8bMmTOjqakp3njjjWHnP/HEE7FixYpoaWmJ3bt3x6OPPhqbNm2KO+644z1vHgCAU1fJofrggw/GTTfdFEuXLo2PfvSjsX79+jj77LPjscceG3b+Cy+8EFdddVXccMMNMWPGjPjUpz4V119//R+9CwsAwOmtpFDt6+uLHTt2RGNj4x+eoKwsGhsbo729fdg1V155ZezYsWMwTPfu3RtbtmyJa6+99j1sGwCAU90ZpUw+ePBg9Pf3R21t7ZDx2tra2LNnz7Brbrjhhjh48GB88pOfjKIo4ujRo3HLLbe866/+e3t7o7e3d/Dnnp6eUrYJAMApYMw/9b9t27ZYvXp1PPzww7Fz58546qmnYvPmzXHvvfced01ra2vU1NQMPurq6sZ6mwAAJDOhKIriRCf39fXF2WefHU8++WQsWLBgcHzJkiVx6NCh+Pd///dj1sybNy8+8YlPxNe//vXBsX/913+Nm2++OX7zm99EWdmxrTzcHdW6urro7u6O6urqE90uAADjpKenJ2pqaka110q6o1pRURGzZ8+Otra2wbGBgYFoa2uLhoaGYde8+eabx8RoeXl5REQcr5ErKyujurp6yAMAgNNLSe9RjYhobm6OJUuWxJw5c2Lu3Lmxdu3aOHLkSCxdujQiIhYvXhzTp0+P1tbWiIiYP39+PPjgg3H55ZdHfX19vPrqq3H33XfH/PnzB4MVAADeqeRQXbhwYRw4cCBWrVoVnZ2dMWvWrNi6devgB6z27ds35A7qXXfdFRMmTIi77rorfvnLX8af/umfxvz58+NrX/va6L0KAABOOSW9R/VkGYv3PAAAMHpO+ntUAQBgvAhVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFIaUaiuW7cuZsyYEVVVVVFfXx/bt29/1/mHDh2KZcuWxdSpU6OysjIuuuii2LJly4g2DADA6eGMUhds2rQpmpubY/369VFfXx9r166NpqamePnll2Py5MnHzO/r64u/+qu/ismTJ8eTTz4Z06dPj1/84hdx7rnnjsb+AQA4RU0oiqIoZUF9fX1cccUV8dBDD0VExMDAQNTV1cWtt94aK1asOGb++vXr4+tf/3rs2bMnzjzzzBFtsqenJ2pqaqK7uzuqq6tH9BwAAIydsei1kn7139fXFzt27IjGxsY/PEFZWTQ2NkZ7e/uwa77//e9HQ0NDLFu2LGpra+OSSy6J1atXR39//3Gv09vbGz09PUMeAACcXkoK1YMHD0Z/f3/U1tYOGa+trY3Ozs5h1+zduzeefPLJ6O/vjy1btsTdd98dDzzwQHz1q1897nVaW1ujpqZm8FFXV1fKNgEAOAWM+af+BwYGYvLkyfHII4/E7NmzY+HChXHnnXfG+vXrj7tm5cqV0d3dPfjYv3//WG8TAIBkSvow1aRJk6K8vDy6urqGjHd1dcWUKVOGXTN16tQ488wzo7y8fHDsIx/5SHR2dkZfX19UVFQcs6aysjIqKytL2RoAAKeYku6oVlRUxOzZs6OtrW1wbGBgINra2qKhoWHYNVdddVW8+uqrMTAwMDj2yiuvxNSpU4eNVAAAiBjBr/6bm5tjw4YN8e1vfzt2794dn//85+PIkSOxdOnSiIhYvHhxrFy5cnD+5z//+fj1r38dt912W7zyyiuxefPmWL16dSxbtmz0XgUAAKeckr9HdeHChXHgwIFYtWpVdHZ2xqxZs2Lr1q2DH7Dat29flJX9oX/r6uri2WefjeXLl8dll10W06dPj9tuuy1uv/320XsVAACcckr+HtWTwfeoAgDkdtK/RxUAAMaLUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkNKIQnXdunUxY8aMqKqqivr6+ti+ffsJrdu4cWNMmDAhFixYMJLLAgBwGik5VDdt2hTNzc3R0tISO3fujJkzZ0ZTU1O88cYb77ru9ddfj3/8x3+MefPmjXizAACcPkoO1QcffDBuuummWLp0aXz0ox+N9evXx9lnnx2PPfbYcdf09/fHZz/72bjnnnviggsueE8bBgDg9FBSqPb19cWOHTuisbHxD09QVhaNjY3R3t5+3HVf+cpXYvLkyXHjjTee0HV6e3ujp6dnyAMAgNNLSaF68ODB6O/vj9ra2iHjtbW10dnZOeya559/Ph599NHYsGHDCV+ntbU1ampqBh91dXWlbBMAgFPAmH7q//Dhw7Fo0aLYsGFDTJo06YTXrVy5Mrq7uwcf+/fvH8NdAgCQ0RmlTJ40aVKUl5dHV1fXkPGurq6YMmXKMfN//vOfx+uvvx7z588fHBsYGPj9hc84I15++eW48MILj1lXWVkZlZWVpWwNAIBTTEl3VCsqKmL27NnR1tY2ODYwMBBtbW3R0NBwzPyLL744Xnzxxejo6Bh8fPrTn45rrrkmOjo6/EofAIDjKumOakREc3NzLFmyJObMmRNz586NtWvXxpEjR2Lp0qUREbF48eKYPn16tLa2RlVVVVxyySVD1p977rkREceMAwDA/6/kUF24cGEcOHAgVq1aFZ2dnTFr1qzYunXr4Aes9u3bF2Vl/sIrAADemwlFURQnexN/TE9PT9TU1ER3d3dUV1ef7O0AAPAOY9Frbn0CAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgpRGF6rp162LGjBlRVVUV9fX1sX379uPO3bBhQ8ybNy8mTpwYEydOjMbGxnedDwAAESMI1U2bNkVzc3O0tLTEzp07Y+bMmdHU1BRvvPHGsPO3bdsW119/ffzoRz+K9vb2qKuri0996lPxy1/+8j1vHgCAU9eEoiiKUhbU19fHFVdcEQ899FBERAwMDERdXV3ceuutsWLFij+6vr+/PyZOnBgPPfRQLF68+ISu2dPTEzU1NdHd3R3V1dWlbBcAgHEwFr1W0h3Vvr6+2LFjRzQ2Nv7hCcrKorGxMdrb20/oOd58881466234rzzzjvunN7e3ujp6RnyAADg9FJSqB48eDD6+/ujtrZ2yHhtbW10dnae0HPcfvvtMW3atCGx+06tra1RU1Mz+KirqytlmwAAnALG9VP/a9asiY0bN8bTTz8dVVVVx523cuXK6O7uHnzs379/HHcJAEAGZ5QyedKkSVFeXh5dXV1Dxru6umLKlCnvuvb++++PNWvWxA9/+MO47LLL3nVuZWVlVFZWlrI1AABOMSXdUa2oqIjZs2dHW1vb4NjAwEC0tbVFQ0PDcdfdd999ce+998bWrVtjzpw5I98tAACnjZLuqEZENDc3x5IlS2LOnDkxd+7cWLt2bRw5ciSWLl0aERGLFy+O6dOnR2tra0RE/PM//3OsWrUqnnjiiZgxY8bge1k/8IEPxAc+8IFRfCkAAJxKSg7VhQsXxoEDB2LVqlXR2dkZs2bNiq1btw5+wGrfvn1RVvaHG7Xf/OY3o6+vL/7mb/5myPO0tLTEl7/85fe2ewAATlklf4/qyeB7VAEAcjvp36MKAADjRagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhJqAIAkJJQBQAgJaEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAApCVUAAFISqgAApCRUAQBISagCAJCSUAUAICWhCgBASkIVAICUhCoAACkJVQAAUhKqAACkJFQBAEhpRKG6bt26mDFjRlRVVUV9fX1s3779Xed/73vfi4svvjiqqqri0ksvjS1btoxoswAAnD5KDtVNmzZFc3NztLS0xM6dO2PmzJnR1NQUb7zxxrDzX3jhhbj++uvjxhtvjF27dsWCBQtiwYIF8bOf/ew9bx4AgFPXhKIoilIW1NfXxxVXXBEPPfRQREQMDAxEXV1d3HrrrbFixYpj5i9cuDCOHDkSP/jBDwbHPvGJT8SsWbNi/fr1J3TNnp6eqKmpie7u7qiuri5luwAAjIOx6LUzSpnc19cXO3bsiJUrVw6OlZWVRWNjY7S3tw+7pr29PZqbm4eMNTU1xTPPPHPc6/T29kZvb+/gz93d3RHx+/8CAADI5+1OK/Ee6LsqKVQPHjwY/f39UVtbO2S8trY29uzZM+yazs7OYed3dnYe9zqtra1xzz33HDNeV1dXynYBABhn//u//xs1NTWj8lwlhep4Wbly5ZC7sIcOHYoPfvCDsW/fvlF74eTV09MTdXV1sX//fm/1OA0479OL8z69OO/TS3d3d5x//vlx3nnnjdpzlhSqkyZNivLy8ujq6hoy3tXVFVOmTBl2zZQpU0qaHxFRWVkZlZWVx4zX1NT4B/00Ul1d7bxPI8779OK8Ty/O+/RSVjZ6335a0jNVVFTE7Nmzo62tbXBsYGAg2traoqGhYdg1DQ0NQ+ZHRDz33HPHnQ8AABEj+NV/c3NzLFmyJObMmRNz586NtWvXxpEjR2Lp0qUREbF48eKYPn16tLa2RkTEbbfdFldffXU88MADcd1118XGjRvjpz/9aTzyyCOj+0oAADillByqCxcujAMHDsSqVauis7MzZs2aFVu3bh38wNS+ffuG3PK98sor44knnoi77ror7rjjjviLv/iLeOaZZ+KSSy454WtWVlZGS0vLsG8H4NTjvE8vzvv04rxPL8779DIW513y96gCAMB4GL13uwIAwCgSqgAApCRUAQBISagCAJBSmlBdt25dzJgxI6qqqqK+vj62b9/+rvO/973vxcUXXxxVVVVx6aWXxpYtW8Zpp4yGUs57w4YNMW/evJg4cWJMnDgxGhsb/+g/H+RS6p/vt23cuDEmTJgQCxYsGNsNMqpKPe9Dhw7FsmXLYurUqVFZWRkXXXSR/01/Hyn1vNeuXRsf/vCH46yzzoq6urpYvnx5/O53vxun3TJSP/7xj2P+/Pkxbdq0mDBhQjzzzDN/dM22bdvi4x//eFRWVsaHPvShePzxx0u/cJHAxo0bi4qKiuKxxx4r/vu//7u46aabinPPPbfo6uoadv5PfvKTory8vLjvvvuKl156qbjrrruKM888s3jxxRfHeeeMRKnnfcMNNxTr1q0rdu3aVezevbv4u7/7u6Kmpqb4n//5n3HeOSNR6nm/7bXXXiumT59ezJs3r/jrv/7r8dks71mp593b21vMmTOnuPbaa4vnn3++eO2114pt27YVHR0d47xzRqLU8/7Od75TVFZWFt/5zneK1157rXj22WeLqVOnFsuXLx/nnVOqLVu2FHfeeWfx1FNPFRFRPP300+86f+/evcXZZ59dNDc3Fy+99FLxjW98oygvLy+2bt1a0nVThOrcuXOLZcuWDf7c399fTJs2rWhtbR12/mc+85niuuuuGzJWX19f/P3f//2Y7pPRUep5v9PRo0eLc845p/j2t789VltkFI3kvI8ePVpceeWVxbe+9a1iyZIlQvV9pNTz/uY3v1lccMEFRV9f33htkVFU6nkvW7as+Mu//MshY83NzcVVV101pvtkdJ1IqH7pS18qPvaxjw0ZW7hwYdHU1FTStU76r/77+vpix44d0djYODhWVlYWjY2N0d7ePuya9vb2IfMjIpqamo47nzxGct7v9Oabb8Zbb70V55133lhtk1Ey0vP+yle+EpMnT44bb7xxPLbJKBnJeX//+9+PhoaGWLZsWdTW1sYll1wSq1evjv7+/vHaNiM0kvO+8sorY8eOHYNvD9i7d29s2bIlrr322nHZM+NntFqt5L+ZarQdPHgw+vv7B/9mq7fV1tbGnj17hl3T2dk57PzOzs4x2yejYyTn/U633357TJs27Zg/AOQzkvN+/vnn49FHH42Ojo5x2CGjaSTnvXfv3vjP//zP+OxnPxtbtmyJV199Nb7whS/EW2+9FS0tLeOxbUZoJOd9ww03xMGDB+OTn/xkFEURR48ejVtuuSXuuOOO8dgy4+h4rdbT0xO//e1v46yzzjqh5znpd1ShFGvWrImNGzfG008/HVVVVSd7O4yyw4cPx6JFi2LDhg0xadKkk70dxsHAwEBMnjw5HnnkkZg9e3YsXLgw7rzzzli/fv3J3hpjYNu2bbF69ep4+OGHY+fOnfHUU0/F5s2b49577z3ZWyOpk35HddKkSVFeXh5dXV1Dxru6umLKlCnDrpkyZUpJ88ljJOf9tvvvvz/WrFkTP/zhD+Oyyy4by20ySko975///Ofx+uuvx/z58wfHBgYGIiLijDPOiJdffjkuvPDCsd00IzaSP99Tp06NM888M8rLywfHPvKRj0RnZ2f09fVFRUXFmO6ZkRvJed99992xaNGi+NznPhcREZdeemkcOXIkbr755rjzzjujrMz9s1PF8Vqturr6hO+mRiS4o1pRURGzZ8+Otra2wbGBgYFoa2uLhoaGYdc0NDQMmR8R8dxzzx13PnmM5LwjIu6777649957Y+vWrTFnzpzx2CqjoNTzvvjii+PFF1+Mjo6OwcenP/3puOaaa6KjoyPq6urGc/uUaCR/vq+66qp49dVXB/+FJCLilVdeialTp4rU5EZy3m+++eYxMfr2v6T8/jM6nCpGrdVK+5zX2Ni4cWNRWVlZPP7448VLL71U3HzzzcW5555bdHZ2FkVRFIsWLSpWrFgxOP8nP/lJccYZZxT3339/sXv37qKlpcXXU72PlHrea9asKSoqKoonn3yy+NWvfjX4OHz48Ml6CZSg1PN+J5/6f38p9bz37dtXnHPOOcU//MM/FC+//HLxgx/8oJg8eXLx1a9+9WS9BEpQ6nm3tLQU55xzTvFv//Zvxd69e4v/+I//KC688MLiM5/5zMl6CZygw4cPF7t27Sp27dpVRETx4IMPFrt27Sp+8YtfFEVRFCtWrCgWLVo0OP/tr6f6p3/6p2L37t3FunXr3r9fT1UURfGNb3yjOP/884uKiopi7ty5xX/9138N/mdXX311sWTJkiHzv/vd7xYXXXRRUVFRUXzsYx8rNm/ePM475r0o5bw/+MEPFhFxzKOlpWX8N86IlPrn+/8nVN9/Sj3vF154oaivry8qKyuLCy64oPja175WHD16dJx3zUiVct5vvfVW8eUvf7m48MILi6qqqqKurq74whe+UPzf//3f+G+ckvzoRz8a9v+L3z7fJUuWFFdfffUxa2bNmlVUVFQUF1xwQfEv//IvJV93QlG41w4AQD4n/T2qAAAwHKEKAEBKQhUAgJSEKgAAKQlVAABSEqoAAKQkVAEASEmoAgCQklAFACAloQoAQEpCFQCAlIQqAAAp/T8cFndlzoQjHAAAAABJRU5ErkJggg==","text/plain":["<Figure size 800x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Some settings for visualisation\n","matplotlib.use('Agg')\n","%matplotlib inline\n","\n","seed = 123\n","torch.manual_seed(seed)\n","\n","# Set parameters (including hyperparameters) and setting for saving/logging data\n","\n","hyper_param_net = training.get_hyperparameter_grid('HuberMC-Net', TrainInstances = 10, ValInstances = 6, BatchSize = 2, ValBatchSize = 2, num_epochs = 5, learning_rate = 0.001)\n","\n","params_net = get_default_param(gpu = False)\n","\n","CalInGPU = params_net['CalInGPU']\n","\n","q_list = [0.2]\n","db_list = [3.0]\n","\n","for q in q_list:\n","  for db in db_list:\n","    # ProjectName = TRY + ' ' + logs_and_results.get_current_time() + ' ' + hyper_param_net['Model'] + ' ' + 'Sampling Rate: ' + logs_and_results.get_q_str(q) + ' and DB ' + logs_and_results.get_noise_str(db)\n","    \n","    ProjectName = TRY + ' ' + hyper_param_net['Model'] + ' ' + 'Sampling Rate: ' + logs_and_results.get_q_str(q) + ' and DB ' + logs_and_results.get_noise_str(db)\n","    # Note: Removed time stamp from log file name as : not supported. Weird because this was not a problem in linux\n","    \n","    # Get log file\n","    logfile = logs_and_results.get_modularized_record(ProjectName, q, db, 'Logs', hyper_param_net, params_net, SESSION)\n","    log = open(logfile, 'w')\n","    print('Project Name: %s'%ProjectName)\n","    log.write('Project Name: %s\\n'%ProjectName)\n","\n","    # Get Model\n","    net = training.get_model(params_net, hyper_param_net, log)\n","    print('Parameters = \\n%s\\n'%str(params_net))\n","    log.write('params_net = \\n%s\\n\\n'%str(params_net))\n","\n","    #Loading data and creating dataloader for both test and training\n","    print('Loading Data phase...')\n","    print('----------------')\n","    log.write('Loading phase...\\n')\n","    log.write('----------------\\n')\n","    shape_dset = (params_net['size1'], params_net['size2'])\n","    \n","    train_loader, val_loader = dataset_processing.get_dataloaders(params_net = params_net, hyper_param_net = hyper_param_net, sampling_rate = q, db = db)\n","\n","    print('Finished loading.\\n')\n","    log.write('Finished loading.\\n\\n');\n","\n","    # Some additional settings for training including loss, optimizer,\n","    # floss = nn.functional.mse_loss(reduction = 'sum')\n","    floss = nn.MSELoss(reduction = 'sum')\n","    optimizer = torch.optim.Adam(net.parameters(), lr = hyper_param_net['Lr'])\n","    # scheduler2 =  torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma = 0.97, verbose = True)\n","\n","    # Array for recording parameter values after each layer for each epoch etc\n","    outputs_L = architecture.to_var(torch.zeros([shape_dset[0], shape_dset[1]]), CalInGPU) \n","    lossmean_vec = np.zeros((hyper_param_net['Epochs'], ))\n","    lossmean_val_vec = np.zeros((hyper_param_net['Epochs'], ))\n","\n","    c_list, lamda_list, mu_list = net.getexp_LS()\n","\n","    c_list_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","    lamda_list_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","    mu_list_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","\n","    # dummy variable to monitor and record progress for loss\n","    minloss = np.inf\n","\n","    for epoch in range(hyper_param_net['Epochs']):\n","      print(f'Epoch: {epoch + 1}, {logs_and_results.get_current_time()}, \\n')\n","      log.write('\\n' + logs_and_results.get_current_time() + '\\n')\n","\n","      # Train and Test Steps. (Record every 5 epochs)\n","      if (epoch + 1) % 5 == 0:\n","          print('Loading and calculating training batches...')\n","          log.write('Loading and calculating training batches...\\n')\n","          startime = time.time()\n","          loss_mean = training.train_step(net, train_loader, floss, optimizer, CalInGPU, hyper_param_net['TrainInstances'], hyper_param_net['BatchSize']) # remove alpha from train func\n","          endtime = time.time()\n","          print('Training time is %f'%(endtime - startime))\n","          log.write('Training time is %f\\n'%(endtime - startime))\n","\n","          print('Loading and calculating validation batches...')\n","          log.write('Loading and calculating validation batches...\\n')\n","          startime = time.time()\n","          loss_val_mean = training.test_step(net, val_loader, floss, CalInGPU, hyper_param_net['ValInstances'], hyper_param_net['ValBatchSize'])\n","          endtime = time.time()\n","          print('Test time is %f'%(endtime - startime))\n","          log.write('Test time is %f\\n'%(endtime - startime))\n","\n","      else:\n","        loss_mean = training.train_step(net, train_loader, floss, optimizer, CalInGPU, hyper_param_net['TrainInstances'], hyper_param_net['BatchSize'])\n","        loss_val_mean = training.test_step(net, val_loader, floss, CalInGPU, hyper_param_net['ValInstances'], hyper_param_net['ValBatchSize'])\n","\n","      # Update Record and Parameters\n","      lossmean_vec[epoch] = loss_mean\n","      lossmean_val_vec[epoch] = loss_val_mean\n","\n","      c_list, lamda_list, mu_list = net.getexp_LS()\n","\n","      c_list_vec[epoch, :] = c_list\n","      lamda_list_vec[epoch, :] = lamda_list\n","      mu_list_vec[epoch, :] = mu_list\n","\n","      print('Epoch [%d/%d], Lossmean:%.5e, Validation lossmean:%.5e'\n","            %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n","      # print('loss_lowrank_mean', loss_lowrank_mean)\n","      # print('loss_val_lowrank_mean', loss_val_lowrank_mean)\n","      print(f'c: {c_list}, lamda: {lamda_list}, mu: {mu_list}')\n","\n","      # Update Log after every 5 epochs. Make a plot of MSE against epochs every 5 epochs. Save Model in whole/dict form every five epochs.\n","      if (epoch + 1) % 5 == 0:\n","        print(f\"Saving Whole Model at Epochs: [{epoch + 1}/{hyper_param_net['Epochs']}]\")\n","        model_whole_path = logs_and_results.get_modularized_record(ProjectName, q, db, 'Saved Models - Whole', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        # torch.save(net, model_whole_path)\n","        print(f\"Saving Model Dict at Epochs: [{epoch + 1}/{hyper_param_net['Epochs']}]\")\n","        model_state_dict_path = logs_and_results.get_modularized_record(ProjectName, q, db, 'Saved Models - Dict', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        # torch.save(net.state_dict(), model_state_dict_path)\n","\n","        # print('Epoch [%d/%d], Lossmean:%.5e, Validation lossmean:%.5e'\n","        # %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n","        # print('loss_lowrank_mean', loss_lowrank_mean)\n","        # print('loss_val_lowrank_mean', loss_val_lowrank_mean)\n","        # print(f'c: {c_list}, lamda: {lamda_list}, mu: {mu_list}')\n","\n","        # log.write('loss_lowrank_mean %.5e\\n' %(loss_lowrank_mean))\n","        # log.write('loss_val_lowrank_mean %.5e\\n' %(loss_val_lowrank_mean))\n","        log.write('Epoch [%d/%d], Lossmean:%.5e, Validation lossmean:%.5e\\n'\n","              %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n","        np.set_printoptions(precision = 3)\n","\n","        log.write('c_list: '+ str(c_list)+'\\n')\n","        log.write('lamda_list: '+ str(lamda_list)+'\\n')\n","        log.write('mu_list: '+ str(mu_list)+'\\n')\n","\n","        if True or loss_val_mean < minloss:\n","          print('saved at [epoch%d/%d]'%(epoch + 1, hyper_param_net['Epochs']))\n","          log.write('saved at [epoch%d/%d]\\n' %(epoch + 1, hyper_param_net['Epochs']))\n","          minloss = min(loss_val_mean, minloss)\n","\n","        # Plotting MSE vs Epoch and Saving it\n","\n","        # Get Directory where we have to save the plot\n","        dir = logs_and_results.get_modularized_record(ProjectName, q, db, 'Plots', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        epochs_vec = np.arange(0, hyper_param_net['Epochs'], 1)\n","        logs_and_results.plot_and_save_mse_vs_epoch(epochs_vec, lossmean_vec, hyper_param_net, lossmean_val_vec, dir, epoch)\n","\n","    # Finish off by observing the minimum loss on validation set\n","\n","    #Print min loss\n","    print('\\nmin Loss = %.4e'%np.min(lossmean_val_vec))\n","    log.write('\\nmin Loss = %.4e\\n'%np.min(lossmean_val_vec))\n","    log.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
