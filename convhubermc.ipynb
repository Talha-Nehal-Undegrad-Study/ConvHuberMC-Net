{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33035,"status":"ok","timestamp":1696836126900,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"QkewbYGcurVf","outputId":"10da4d48-8e12-4001-d652-864b9e977ded"},"outputs":[],"source":["# Deep Learning Libraries\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.utils.data as data\n","\n","# Data Manipulation and Analysis\n","import numpy as np\n","import pandas as pd\n","import collections # A module providing alternative data structures like named tuples, defaultdict, Counter, etc., compared to built-in Python containers.\n","import random\n","\n","# Data Visualization\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn\n","\n","# File and System Interaction\n","import glob\n","import os\n","from pathlib import Path\n","import shutil\n","\n","# Scientific Computing and Math\n","import math\n","import cmath\n","\n","# Date and Time Handling\n","import time\n","import datetime\n","\n","# Linear Algebra\n","from torch import linalg as LA\n","\n","# Neural Architecture\n","from torchinfo import summary\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from python_scripts import generate_synthetic_data\n","from python_scripts import format_data\n","from python_scripts import dataset_processing\n","from python_scripts import forward_pass\n","from python_scripts import architecture"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":327},"executionInfo":{"elapsed":11,"status":"error","timestamp":1696836126901,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"79B6gFe38zuq","outputId":"f692e85f-826d-4670-a27b-60828627bf42"},"outputs":[{"name":"stdout","output_type":"stream","text":["M_train.shape: (40, 150, 300), M_Omega_train.shape: (40, 150, 300), M_test.shape: (20, 150, 300), M_Omega_test.shape: (20, 150, 300)\n"]}],"source":["# Generate Data\n","M_train, M_Omega_train, M_test, M_Omega_test = generate_synthetic_data.generate(150, 300, 10, 40, 20, 0.45, 9)\n","\n","print(f'M_train.shape: {M_train.shape}, M_Omega_train.shape: {M_Omega_train.shape}, M_test.shape: {M_test.shape}, M_Omega_test.shape: {M_Omega_test.shape}')\n","\n","# Format and Save Data\n","format_data.format(M_train, M_Omega_train, M_test, M_Omega_test)\n","\n","# Create DataLoaders\n","train_dataset = dataset_processing.ImageDataset(40, (150, 300), 0)\n","train_loader = data.DataLoader(train_dataset, batch_size = 5, shuffle = True)\n","test_dataset = dataset_processing.ImageDataset(5, (150, 300), 1)\n","test_loader = data.DataLoader(test_dataset, batch_size = 5, shuffle = True)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([5, 150, 300]) torch.Size([5, 150, 300])\n","torch.Size([5, 150, 300]) torch.Size([5, 150, 300])\n","torch.Size([5, 150, 300]) torch.Size([5, 150, 300])\n","torch.Size([5, 150, 300]) torch.Size([5, 150, 300])\n","torch.Size([5, 150, 300]) torch.Size([5, 150, 300])\n","torch.Size([5, 150, 300]) torch.Size([5, 150, 300])\n","torch.Size([5, 150, 300]) torch.Size([5, 150, 300])\n","torch.Size([5, 150, 300]) torch.Size([5, 150, 300])\n"]}],"source":["for data in train_loader:\n","    print(data[0].shape, data[1].shape)"]},{"cell_type":"markdown","metadata":{"id":"24abWA7yyU5Q"},"source":["#### DataSet_Unfolded for Real World Sensing Data.py"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1696836126902,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"zwYsPMqG68M6"},"outputs":[],"source":["# Setting up some global variables\n","\n","ROOT = 'C:/Users/Talha/OneDrive - Higher Education Commission/Documents/GitHub/ConvHuberMC/HuberMC_Data'\n","TRY = '1st try'\n","SESSION = 'Session 1'"]},{"cell_type":"markdown","metadata":{"id":"reS6gl1b1M70"},"source":["#### Model path of loading"]},{"cell_type":"markdown","metadata":{"id":"RGsjtUu7-iJ5"},"source":["Testing Training Loop"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1696836126902,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"nOIBCr5gIr8Y"},"outputs":[],"source":["# Get parameters --> for convhubermc: c, lambda, sigma, mu, delta, tau\n","def get_default_param(gpu = True):\n","    params_net = {}\n","    params_net['layers'] = 6\n","    params_net['rank'] = 10\n","\n","    params_net['initial_c'] = 1.345\n","    params_net['initial_lamda'] = 0.50\n","    params_net['initial_sigma'] = 0.67 # may not be learnable as then tau and lamda no longer needed\n","\n","    params_net['initial_mu'] = 0.10\n","    params_net['initial_w'] = 0.2\n","\n","    params_net['initial_delta'] = 0.40\n","    params_net['initial_tau'] = 0.21\n","\n","    params_net['CalInGPU'] = gpu #whether to calculate in GPU\n","    params_net['iter'] = 5\n","    params_net['size1'] = 150\n","    params_net['size2'] = 300\n","    params_net['rank'] = 10\n","    return params_net"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["UnfoldedNet_Huber(\n","  (huber_obj): Huber(\n","    (relu): ReLU()\n","    (sigmoid): Sigmoid()\n","  )\n","  (sig): Sigmoid()\n","  (relu): ReLU()\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Testing UnfoldedNetHuber\n","params_net = get_default_param()\n","model = architecture.UnfoldedNet_Huber(params = params_net)\n","model"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["D, L = next(iter(train_loader))\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["V Multiprocess Done! \n","\n"]},{"ename":"AttributeError","evalue":"'generator' object has no attribute 'detach'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# summary(model, input_size = [150, 300], col_names = ['input_size', 'output_size', 'num_params'])\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\Talha\\OneDrive - Higher Education Commission\\Documents\\GitHub\\ConvHuberMC\\python_scripts\\architecture.py:53\u001b[0m, in \u001b[0;36mUnfoldedNet_Huber.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Now initalize the neural architecture of even number of layers where even numbered layers correspond to updating U and odd numbered\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Step 1: Compute Forward Pass through all the layers and predict ground truth matrix\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     pred_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhuber_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred_matrix\n","File \u001b[1;32mc:\\Users\\Talha\\OneDrive - Higher Education Commission\\Documents\\GitHub\\ConvHuberMC\\python_scripts\\forward_pass.py:81\u001b[0m, in \u001b[0;36mHuber.forward\u001b[1;34m(self, U, V, X)\u001b[0m\n\u001b[0;32m     78\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhubreg, argslist_v)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV Multiprocess Done! \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()))\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV[:, idx] \u001b[38;5;241m=\u001b[39m result\n","\u001b[1;31mAttributeError\u001b[0m: 'generator' object has no attribute 'detach'"]}],"source":["# summary(model, input_size = [150, 300], col_names = ['input_size', 'output_size', 'num_params'])\n","model.forward(D[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":653},"executionInfo":{"elapsed":415,"status":"error","timestamp":1696360284646,"user":{"displayName":"Talha Ahmed","userId":"17372384323512083426"},"user_tz":-300},"id":"bmMuNqIs97hm","outputId":"ebed6b71-2dc9-42fe-b8e7-399ecf985c27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Project Name: 5st try 2023-10-04 00:11:23 ConvMC-Net Sampling Rate: 50.0% and Noise Variance 3.0\n","Configuring Network...\n","\n","Instantiating Model...\n","\n","Model Instantiated...\n","\n","Parameters = \n","{'layers': 5, 'kernel': [(3, 1), (3, 1), (3, 1), (3, 1), (3, 1)], 'initial_mu_inverse': 0.0, 'initial_y1': 0.8, 'coef_mu_inverse': 0.36, 'CalInGPU': True, 'size1': 400, 'size2': 500}\n","\n","Loading Data phase...\n","----------------\n","Finished loading.\n","\n","Adjusting learning rate of group 0 to 1.2000e-02.\n","Epoch: 1, 2023-10-04 00:11:23, \n","\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-ca354f222db4>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m           \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test time is %f\\n'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_lowrank_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCalInGPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_param_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_param_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TrainInstances'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_param_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BatchSize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mloss_val_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val_lowrank_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCalInGPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_param_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_param_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ValInstances'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_param_net\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ValBatchSize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/py_scripts/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, CalInGPU, Alpha, TrainInstances, batch, inference)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/py_scripts/convmc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/py_scripts/convmc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, lst)\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (500) must match the size of tensor b (60) at non-singleton dimension 1"]}],"source":["# Some settings for visualisation\n","matplotlib.use('Agg')\n","%matplotlib inline\n","\n","seed = 123\n","torch.manual_seed(seed)\n","\n","# Set parameters (including hyperparameters) and setting for saving/logging data\n","\n","hyper_param_net = training.get_hyperparameter_grid('HuberMC-Net', TrainInstances = 40, ValInstances = 20, BatchSize = 4, ValBatchSize = 4, num_epochs = 40, learning_rate = 0.0012)\n","\n","params_net = get_default_param(True)\n","\n","CalInGPU = params_net['CalInGPU']\n","\n","q_list = [0.45]\n","db_list = [9.0]\n","\n","for q in q_list:\n","  for db in db_list:\n","    ProjectName = TRY + ' ' + logs_and_results.get_current_time() + ' ' + hyper_param_net['Model'] + ' ' + 'Sampling Rate: ' + logs_and_results.get_q_str(q) + ' and DB ' + logs_and_results.get_noise_str(db)\n","\n","    # Get log file\n","    logfile = logs_and_results.get_modularized_record(ProjectName, q, db, 'Logs', hyper_param_net, params_net, SESSION)\n","    log = open(logfile, 'w')\n","    print('Project Name: %s'%ProjectName)\n","    log.write('Project Name: %s\\n'%ProjectName)\n","\n","    # Get Model\n","    net = training.get_model(params_net, hyper_param_net, log)\n","    print('Parameters = \\n%s\\n'%str(params_net))\n","    log.write('params_net=\\n%s\\n\\n'%str(params_net))\n","\n","    #Loading data and creating dataloader for both test and training\n","    print('Loading Data phase...')\n","    print('----------------')\n","    log.write('Loading phase...\\n')\n","    log.write('----------------\\n')\n","    shape_dset = (params_net['size1'], params_net['size2'])\n","    \n","    train_dataset = dataset_processing.ImageDataset(round(hyper_param_net['TrainInstances']), shape_dset, 0, q, db)\n","    train_loader = data.DataLoader(train_dataset,batch_size = hyper_param_net['BatchSize'], shuffle = True)\n","    val_dataset = dataset_processing.ImageDataset(round(hyper_param_net['ValInstances']), shape_dset, 1, q, db)\n","    test_loader = data.DataLoader(val_dataset, batch_size = hyper_param_net['ValBatchSize'], shuffle = True)\n","\n","    print('Finished loading.\\n')\n","    log.write('Finished loading.\\n\\n');\n","\n","    # Some additional settings for training including loss, optimizer,\n","    floss = nn.MSELoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr = hyper_param_net['Lr'])\n","    scheduler2 =  torch.optim.lr_scheduler.StepLR(optimizer, step_size= 1, gamma = 0.97, verbose = True)\n","\n","    # Array for recording parameter values after each layer for each epoch etc\n","    outputs_L = hubermc.to_var(torch.zeros([shape_dset[0], shape_dset[1]]), CalInGPU) \n","    lossmean_vec = np.zeros((hyper_param_net['Epochs'], ))\n","    lossmean_val_vec = np.zeros((hyper_param_net['Epochs'], ))\n","\n","    lamda, mu, c, sigma, tau = net.getexp_LS()\n","\n","    lamda_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","    mu_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","    c_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","    db_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","    tau_vec = np.zeros((hyper_param_net['Epochs'], net.layers))\n","\n","    # dummy variable to monitor and record progress for loss\n","    minloss = np.inf\n","\n","    for epoch in range(hyper_param_net['Epochs']):\n","      print(f'Epoch: {epoch + 1}, {logs_and_results.get_current_time()}, \\n')\n","      log.write('\\n' + logs_and_results.get_current_time() + '\\n')\n","\n","      # Train and Test Steps. (Record every 5 epochs)\n","      if (epoch + 1) % 5 == 0:\n","          print('Loading and calculating training batches...')\n","          log.write('Loading and calculating training batches...\\n')\n","          startime = time.time()\n","          loss_mean, loss_lowrank_mean = training.train_step(net, train_loader, floss, optimizer, CalInGPU, hyper_param_net['TrainInstances'], hyper_param_net['BatchSize']) # remove alpha from train func\n","          endtime = time.time()\n","          print('Training time is %f'%(endtime - startime))\n","          log.write('Training time is %f\\n'%(endtime - startime))\n","\n","          print('Loading and calculating validation batches...')\n","          log.write('Loading and calculating validation batches...\\n')\n","          startime = time.time()\n","          loss_val_mean, loss_val_lowrank_mean = training.test_step(net, test_loader, floss, CalInGPU, hyper_param_net['ValInstances'], hyper_param_net['ValBatchSize'])\n","          endtime = time.time()\n","          print('Test time is %f'%(endtime - startime))\n","          log.write('Test time is %f\\n'%(endtime - startime))\n","\n","      else:\n","        loss_mean, loss_lowrank_mean = training.train_step(net, train_loader, floss, optimizer, CalInGPU, hyper_param_net['TrainInstances'], hyper_param_net['BatchSize'])\n","        loss_val_mean, loss_val_lowrank_mean = training.test_step(net, test_loader, floss, CalInGPU, hyper_param_net['ValInstances'], hyper_param_net['ValBatchSize'])\n","\n","      # Update Record and Parameters\n","      lossmean_vec[epoch] = loss_mean\n","      lossmean_val_vec[epoch] = loss_val_mean\n","\n","      lamda, mu, c, sigma, tau = net.getexp_LS()\n","\n","      lamda_vec[epoch, :] = lamda\n","      mu_vec[epoch, :] = mu\n","      c_vec[epoch, :] = c\n","      db_vec[epoch, :] = db\n","      tau_vec[epoch, :] = tau\n","\n","      # Update Log after every 5 epochs. Make a plot of MSE against epochs every 5 epochs. Save Model in whole/dict form every five epochs.\n","      if (epoch + 1) % 5 == 0:\n","        print(f\"Saving Whole Model at Epochs: [{epoch + 1}/{hyper_param_net['Epochs']}]\")\n","        model_whole_path = logs_and_results.get_modularized_record(ProjectName, q, db, 'Saved Models - Whole', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        torch.save(net, model_whole_path)\n","        print(f\"Saving Model Dict at Epochs: [{epoch + 1}/{hyper_param_net['Epochs']}]\")\n","        model_state_dict_path = logs_and_results.get_modularized_record(ProjectName, q, db, 'Saved Models - Dict', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        torch.save(net.state_dict(), model_state_dict_path)\n","\n","        print('Epoch [%d/%d], Lossmean:%.5e, Validation lossmean:%.5e'\n","              %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n","        print('loss_lowrank_mean', loss_lowrank_mean)\n","        print('loss_val_lowrank_mean', loss_val_lowrank_mean)\n","\n","        log.write('loss_lowrank_mean %.5e\\n' %(loss_lowrank_mean))\n","        log.write('loss_val_lowrank_mean %.5e\\n' %(loss_val_lowrank_mean))\n","        log.write('Epoch [%d/%d], Lossmean:%.5e, Validation lossmean:%.5e\\n'\n","              %(epoch + 1, hyper_param_net['Epochs'], loss_mean, loss_val_mean))\n","        np.set_printoptions(precision = 3)\n","\n","        log.write('lamda: '+ str(lamda)+'\\n')\n","        log.write('mu: '+ str(mu)+'\\n')\n","        log.write('c: '+ str(c)+'\\n')\n","        log.write('sigma: '+ str(sigma)+'\\n')\n","        log.write('tau: '+ str(tau)+'\\n')\n","\n","        if True or loss_val_mean<minloss:\n","          print('saved at [epoch%d/%d]'%(epoch + 1, hyper_param_net['Epochs']))\n","          log.write('saved at [epoch%d/%d]\\n' %(epoch + 1, hyper_param_net['Epochs']))\n","          minloss = min(loss_val_mean, minloss)\n","\n","        # Plotting MSE vs Epoch and Saving it\n","\n","        # Get Directory where we have to save the plot\n","        dir = logs_and_results.get_modularized_record(ProjectName, q, db, 'Plots', hyper_param_net, params_net, SESSION, current_epoch = epoch + 1)\n","        epochs_vec = np.arange(0, hyper_param_net['Epochs'], 1)\n","        logs_and_results.plot_and_save_mse_vs_epoch(epochs_vec, lossmean_vec, hyper_param_net, lossmean_val_vec, dir, epoch)\n","\n","    # Finish off by observing the minimum loss on validation set\n","\n","    #Print min loss\n","    print('\\nmin Loss = %.4e'%np.min(lossmean_val_vec))\n","    log.write('\\nmin Loss = %.4e\\n'%np.min(lossmean_val_vec))\n","    log.close()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
